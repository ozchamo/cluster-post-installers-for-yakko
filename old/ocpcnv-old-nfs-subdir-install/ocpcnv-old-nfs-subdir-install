#!/bin/bash
# Instructions for CLI install of CNV at:
# https://docs.redhat.com/en/documentation/openshift_container_platform/4.15/html/virtualization/installing#installing-virt-operator-cli_installing-virt

#==========================================================================================
# USER DEFINED PARAMETERS:
#==========================================================================================
#
# These four parameters can be user defined:
#
# NFSSERVER can be YOUR NFS server, else YAKKO will enable NFS on the YAKKO host and set this address up automatically
NFSSERVER=""     
#
# This is the directory offered for use on the NFS server. 
# If empty, YAKKO will create a share in the YAKKO folder with name $VMSTORAGENAME
# If not empty, it will be mounted from NFSSERVER, or will be created on the YAKKO host
NFSPATH=""
VMSTORAGENAME=vm-nfs-storage # ignored if the above is not blank. Change to your liking as above
#
# if you want to break up the install of OCPCNV by stages, change to N
AUTO=Y

# Examples?
# 1) Leave both blank and YAKKO will create a NFS share on the YAKKO host on BASENETWORK.1 
#    this is the YAKKO host as seen in the virtual network
# 2) NFSSERVER=192.168.1.10
#    NFSPATH=/mnt/openshiftvirtvms
#    The above indicates that you will be using YOUR NFS server 192.168.1.10 and there is an 
#    existing share already created at /mnt/openshiftvirtvms
#    NOTE that both have to exist if you are using your own NFS server. Refer to the below for setting up
# 3) NFSSERVER=""
#    NFSPATH=/mnt/myvms
#    YAKKO will create a NFS share on /mnt/myvms on the YAKKO host on BASENETWORK.1
#
#==========================================================================================

process-stage() {

        #$1 is wait time
        #$* is stage name
        WAITTIME=$1
        shift
        STAGENAME=$*

	echo
	echo "--------------------------------------------------------------------"
	if [ "$AUTO"  == Y ]
	then
                echo "Proceeding with stage [$STAGENAME]"
		echo
		return 0
	fi

        while true
        do
                echo
                echo -n "Proceed with stage [$STAGENAME] [(Y)es/(N)o/(E)xit]? "
 		read RESPONSE

                if [ "$RESPONSE" == "Y" ]
                then
                        return 0
                        break
                fi

                if [ "$RESPONSE" == "N" ]
                then
                        return 1
                        break
                fi

                if [ "$RESPONSE" == "E" ]
                then
                        exit
                fi
        done

        sleep $WAITTIME
}


setup-nfs-provisioner() {
	# https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner#manually

	# Reference vars if you want to change the names
	NFS_STORAGE_CLASS_NAME=nfs-client # the test pod uses this name, so only change if not using it!!
	NFSNAMESPACE=nfs-provisioner

	if [ $# -ne 2 ]
	then
		echo
		echo "USAGE: $(basename $0) server path" 
		echo "       This script simplifies the configuration of a NFS provisioner and storage class"
		echo "       - progress --> will create all required assets in the indicated path  and proceed to configure"
		echo "                      if passed, server and path are the NFS resource to use on the nework"
		exit 1
	else
		NFS_SERVER=$1
		NFS_PATH=$2
	fi

	# First lets test the path!
	showmount -e ${NFS_SERVER} 2>/dev/null | grep "${NFS_PATH} " >/dev/null 
	if [ $? -eq 0 ]
	then
		echo
		echo "Path ${NFS_SERVER}:/${NFS_PATH} is available at the NFS server."
		echo "NOTE: This script does not check for correct permissions."
		echo
	else
		echo "ERROR: Path ${NFS_SERVER}:/${NFS_PATH} does not appear to be exported."
		echo "       Check it and run again."
		exit
	fi

	# Now we set up the deployment.yaml file
	${OC} new-project ${NFSNAMESPACE}
	${OC} adm policy add-scc-to-user hostmount-anyuid system:serviceaccount:$NFSNAMESPACE:nfs-client-provisioner
	${OC} create -f - <<HEREDOC
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  namespace: ${NFSNAMESPACE}
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: ${NFSNAMESPACE}
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: ${NFSNAMESPACE}
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: ${NFSNAMESPACE}
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: ${NFSNAMESPACE}
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
HEREDOC

	${OC} apply -f - <<HEREDOC
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: ${NFS_SERVER}
            - name: NFS_PATH
              value: ${NFS_PATH}
      volumes:
        - name: nfs-client-root
          nfs:
            server: ${NFS_SERVER}
            path: ${NFS_PATH}
HEREDOC

	${OC} apply -f - <<HEREDOC
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ${NFS_STORAGE_CLASS_NAME}
  annotations:
    storage.kubernetes.io/is-default-class: "true"
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'
reclaimPolicy: Delete
allowVolumeExpansion: true
parameters:
  pathPattern: "\${.PVC.namespace}/\${.PVC.name}" # waits for nfs.io/storage-path annotation, if not specified will accept as empty string.
  onDelete: delete
---
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: nfs-snapclass
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: "true"
driver: k8s-sigs.io/nfs-subdir-external-provisioner
deletionPolicy: Delete
parameters:
HEREDOC

	${OC} patch storageclass ${NFS_STORAGE_CLASS_NAME} -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
	
	return 0
}

# ========================================================================
# THE INSTALLER STARTS HERE 
# ========================================================================

# a couple of simple checks

./yakko infra snaptake

if [ "$#" -ne 0  ]
then 
	echo
	echo "USAGE $0 [NFS-PATH [NFS-SERVER]]"
	echo "      if not provided, NFS-SERVER will be resolved as the YAKKO host in the virtual network"
	echo "      and NFS-PATH will default to 'YAKKO-DIR/$NFSPATH"
	echo
	exit 1
fi

OC=$(find . -name oc)
if [ -z "${OC}" ] 
then
	OC=$(which oc)
	if [ $? -ne 0 ]
	then
		echo "'oc' command not available"
		exit 1
	fi
fi

if [ -z "$NFSSERVER" ]
then
	if [ -z ".yakkohome" ] 
	then
		echo "ERROR: Automated installation depends on a cluster built with YAKKO"
		echo "       Edit NFSSERVER at the top for your own NFS server"
		exit 1
	fi

	if [ -z "$NFSPATH" ]
	then
		# The VM dir will be setup on the YAKKO directory
		NFSPATH=$(realpath .)/${VMSTORAGENAME}
	fi

	NFSSERVER=$(cat .lastclusterbuild | grep BASENETWORK | cut -f2 -d=).1

	echo "- NFS Path has been configured as $NFSPATH"
	echo "- NFS Server has been configured as $NFSSERVER"
	process-stage 1 "Setup YAKKO host NFS share" && ./yakko infra nfsshare $NFSPATH 
else
	if [ -z "NFSPATH" ]
	then
		echo "ERROR: You have configured an NFS server without specifying a path"
		exit
	fi 
	echo "- NFS Path has been configured as $NFSPATH"
	echo "- NFS Server has been configured as $NFSSERVER"
fi

process-stage 1 "Cluster Login" && {
	if [ -x ./cluster-login ]
	then 
		./cluster-login
	else
		${OC} login
	fi
}	

process-stage 5 "NFS Provisioner" && setup-nfs-provisioner $NFSSERVER $NFSPATH

process-stage 60 "Subscribe OCP CNV Operator" && {
${OC} apply -f - <<HEREDOC
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-cnv
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: kubevirt-hyperconverged-group
  namespace: openshift-cnv
spec:
  targetNamespaces:
    - openshift-cnv
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: hco-operatorhub
  namespace: openshift-cnv
spec:
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  name: kubevirt-hyperconverged
  startingCSV: kubevirt-hyperconverged-operator.v4.15.2
  channel: "stable" 
HEREDOC

	while true 
	do
		${OC} get csv -n openshift-cnv 2>/dev/null | grep "Succeeded" > /dev/null
		[ $? -eq 0 ] && break
		sleep 10
	done
}

process-stage 10 "Deploy OCP CNV/HyperConverged Instance" && {
	${OC} apply -f - <<HEREDOC
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
HEREDOC

	${OC} project default

	while true 
	do
		${OC} events HyperConverged kubevirt-hyperconverged -n openshift-cnv | grep "Created container virt-handler" > /dev/null
		[ $? -eq 0 ] && break
		sleep 10
	done
	sleep 30
}

process-stage 30 "Install Hostpath Provisioner (HPP)" && ${OC} apply -f - <<HEREDOC
apiVersion: hostpathprovisioner.kubevirt.io/v1beta1
kind: HostPathProvisioner
metadata:
  name: hostpath-provisioner
spec:
  imagePullPolicy: IfNotPresent
  storagePools: 
  - name: cnv-host-path 
    path: "/var/cnv-volumes" 
workload:
  nodeSelector:
    kubernetes.io/os: linux
HEREDOC

process-stage 60 "Install Hostpath CSI" && ${OC} apply -f - <<HEREDOC
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hostpath-csi
provisioner: kubevirt.io.hostpath-provisioner
reclaimPolicy: Delete 
volumeBindingMode: WaitForFirstConsumer 
parameters:
  storagePool: cnv-host-path
HEREDOC


